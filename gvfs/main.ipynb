{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from mazelab.generators import random_maze\n",
    "\n",
    "from actor import Actor\n",
    "from agent import Agent\n",
    "from environment import Env, maze_config\n",
    "from tabular import GVFTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "MAZE_SIZE = [10, 10]\n",
    "MAZE_COMPLEXITY = 0.6  # measures complexity and density of walls in maze\n",
    "MAX_STEPS_MAZE = 200\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "NUM_EPOCHS = 1500\n",
    "CHANGE_MAZE_FREQ = 17\n",
    "NUM_EXPLO_EPISODES = 15\n",
    "BATCH_SIZE = 300\n",
    "\n",
    "NUM_LEVELS_GVF = 6\n",
    "GAMMA_GVFS = 0.99\n",
    "LR_GVFS = 0.5\n",
    "NUM_PRIMITIVES = 4\n",
    "LR_ACTOR = 1e-3\n",
    "\n",
    "EPSILON_EXPLO = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment already registered\n"
     ]
    }
   ],
   "source": [
    "# Register environment in gym:\n",
    "try:\n",
    "    gym.envs.register(\n",
    "        id=\"RandomMaze-v0\", entry_point=Env, max_episode_steps=MAX_STEPS_MAZE\n",
    "    )\n",
    "except gym.error.Error as e:\n",
    "    print(\"Environment already registered\")\n",
    "\n",
    "env = gym.make(\"RandomMaze-v0\")\n",
    "\n",
    "# INITIALIZE Agent, GVFs and Actor\n",
    "agent = Agent(env, EPSILON_EXPLO)\n",
    "gvfs = GVFTable(MAZE_SIZE, NUM_LEVELS_GVF, NUM_ACTIONS, GAMMA_GVFS, LR_GVFS)\n",
    "actor = Actor(1, NUM_ACTIONS, NUM_PRIMITIVES)\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=LR_ACTOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0ccf7a71b0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgvfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/gvfs/agent.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, env, gvfs, actor, actor_optimizer, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m             )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mprimitives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgvfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_primitives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 primitives[prim, :] = (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "total_reward = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    clear_output(wait=True)\n",
    "    if i == 0 or i % CHANGE_MAZE_FREQ == 0:\n",
    "        gvfs.reset()  # Reset GVFs to zero for the new maze\n",
    "\n",
    "        # Set parameters for the new maze and create new one\n",
    "\n",
    "        maze_config[\"shape\"] = random_maze(\n",
    "            width=MAZE_SIZE[0],\n",
    "            height=MAZE_SIZE[1],\n",
    "            complexity=MAZE_COMPLEXITY,\n",
    "            density=MAZE_COMPLEXITY,\n",
    "        )\n",
    "        env = gym.make(\"RandomMaze-v0\")\n",
    "        L = env.maze.objects.free.positions\n",
    "        maze_config[\"goal_idx\"] = [\n",
    "            L[np.random.randint(0, len(L))]\n",
    "        ]  # random goal position\n",
    "\n",
    "    # first episode starts at reward location to ease exploration, otherwise start is random\n",
    "    if i == 1 or i == 2 or any((i - t) % CHANGE_MAZE_FREQ == 0 for t in [1, 2]):\n",
    "        maze_config[\"start_idx\"] = maze_config[\"goal_idx\"]\n",
    "    else:\n",
    "        maze_config[\"start_idx\"] = [L[np.random.randint(0, len(L))]]\n",
    "\n",
    "    if i < NUM_EXPLO_EPISODES or any(\n",
    "        (i - t) % CHANGE_MAZE_FREQ == 0 for t in range(NUM_EXPLO_EPISODES)\n",
    "    ):\n",
    "        agent.exploration = True\n",
    "    else:\n",
    "        agent.exploration = False\n",
    "\n",
    "    _ = env.reset()\n",
    "\n",
    "    performance = agent.train_one_epoch(env, gvfs, actor, actor_optimizer, BATCH_SIZE)\n",
    "\n",
    "    if agent.exploration is False:\n",
    "        total_reward.append(performance)\n",
    "\n",
    "    # print(len(env.motions))\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Episode: {i}     Reward:{performance} \")\n",
    "    print(np.round_(gvfs.primitives, decimals=2))\n",
    "    # print(\"\\n\" * 10)\n",
    "\n",
    "\n",
    "    plt.plot(total_reward); plt.show()\n",
    "\n",
    "    if (i+1) % CHANGE_MAZE_FREQ == 0:\n",
    "        plt.figure(dpi=100)\n",
    "        idx = 0\n",
    "        for level in range(gvfs.num_levels):\n",
    "            for prim in range(gvfs.num_primitives):\n",
    "                idx += 1\n",
    "                plt.subplot(gvfs.num_levels, gvfs.num_actions, idx)\n",
    "                plt.imshow(gvfs.values[:, :, level, prim])\n",
    "                if level==0:\n",
    "                    plt.title(np.round_(gvfs.primitives[prim,:], decimals=1),fontsize=5)\n",
    "                plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec457f50f238c1b9be69a8140a28e2ee46cea2881319ad47d01402b494c884e7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('virtEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
